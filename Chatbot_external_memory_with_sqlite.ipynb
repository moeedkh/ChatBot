{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "af6c7afe-1037-41ab-98e4-494692e47402",
      "metadata": {
        "id": "af6c7afe-1037-41ab-98e4-494692e47402"
      },
      "source": [
        "# Chatbot with message summarization & external DB memory\n",
        "\n",
        "## Review\n",
        "\n",
        "We've covered how to customize graph state schema and reducer.\n",
        "\n",
        "We've also shown a number of tricks for trimming or filtering messages in graph state.\n",
        "\n",
        "We've used these concepts in a Chatbot with memory that produces a running summary of the conversation.\n",
        "\n",
        "## Goals\n",
        "\n",
        "But, what if we want our Chatbot to have memory that persists indefinitely?\n",
        "\n",
        "Now, we'll introduce some more advanced checkpointers that support external databases.\n",
        "\n",
        "Here, we'll show how to use [Sqlite as a checkpointer](https://langchain-ai.github.io/langgraph/concepts/low_level/#checkpointer), but other checkpointers, such as [Postgres](https://langchain-ai.github.io/langgraph/how-tos/persistence_postgres/) are available!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "85ed78d9-6ca2-45ac-96a9-52e341ec519d",
      "metadata": {
        "id": "85ed78d9-6ca2-45ac-96a9-52e341ec519d"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph-checkpoint-sqlite langchain_core langgraph langchain_google_genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2e10c4d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e10c4d4",
        "outputId": "c25d2e98-a13e-4d24-f621-34b4a39aa4aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: GOOGLE_API_KEY={userdata.get('GEMINI_API_KEY')}\n",
            "{userdata.get('GEMINI_API_KEY')}\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "%env GOOGLE_API_KEY = {userdata.get('GEMINI_API_KEY')}\n",
        "\n",
        "import os\n",
        "print(os.environ[\"GOOGLE_API_KEY\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6a002315",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a002315",
        "outputId": "d30101b7-08cb-47e0-cab8-b4d0ffad7b38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: LANGCHAIN_API_KEY={userdata.get('LANGCHAIN_API_KEY')}\n"
          ]
        }
      ],
      "source": [
        "%env LANGCHAIN_API_KEY = {userdata.get('LANGCHAIN_API_KEY')}\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-academy\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b40d25c0-e9b5-4854-bf07-3cc3ff07122e",
      "metadata": {
        "id": "b40d25c0-e9b5-4854-bf07-3cc3ff07122e"
      },
      "source": [
        "## Sqlite\n",
        "\n",
        "A good starting point here is the [SqliteSaver checkpointer](https://langchain-ai.github.io/langgraph/concepts/low_level/#checkpointer).\n",
        "\n",
        "Sqlite is a [small, fast, highly popular](https://x.com/karpathy/status/1819490455664685297) SQL database.\n",
        "\n",
        "If we supply `\":memory:\"` it creates an in-memory Sqlite database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fae15402-17ae-4e89-8ecf-4c89e08b22fe",
      "metadata": {
        "id": "fae15402-17ae-4e89-8ecf-4c89e08b22fe"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "# In memory\n",
        "conn = sqlite3.connect(\":memory:\", check_same_thread = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2bf53ec-6d4a-42ce-8183-344795eed403",
      "metadata": {
        "id": "c2bf53ec-6d4a-42ce-8183-344795eed403"
      },
      "source": [
        "But, if we supply a db path, then it will create a database for us!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "58339167-920c-4994-a0a7-0a9c5d4f7cf7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58339167-920c-4994-a0a7-0a9c5d4f7cf7",
        "outputId": "22cbcc9f-e206-4a62-8306-fa08ffc193c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-29 14:51:29--  https://github.com/langchain-ai/langchain-academy/raw/main/module-2/state_db/example.db\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/langchain-ai/langchain-academy/main/module-2/state_db/example.db [following]\n",
            "--2024-12-29 14:51:29--  https://raw.githubusercontent.com/langchain-ai/langchain-academy/main/module-2/state_db/example.db\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 110592 (108K) [application/octet-stream]\n",
            "Saving to: ‚Äòstate_db/example.db‚Äô\n",
            "\n",
            "example.db          100%[===================>] 108.00K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-12-29 14:51:29 (2.95 MB/s) - ‚Äòstate_db/example.db‚Äô saved [110592/110592]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# pull file if it doesn't exist and connect to local db\n",
        "!mkdir -p state_db && [ ! -f state_db/example.db ] && wget -P state_db https://github.com/langchain-ai/langchain-academy/raw/main/module-2/state_db/example.db\n",
        "\n",
        "db_path = \"state_db/example.db\"\n",
        "conn = sqlite3.connect(db_path, check_same_thread=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3c7736b6-a750-48f8-a838-8e7616b12250",
      "metadata": {
        "id": "3c7736b6-a750-48f8-a838-8e7616b12250"
      },
      "outputs": [],
      "source": [
        "# Here is our checkpointer\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "memory: SqliteSaver = SqliteSaver(conn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
        "from typing import Literal, Union\n",
        "from langgraph.graph import END\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "model: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(model = \"gemini-1.5-flash\")\n",
        "\n",
        "class State(MessagesState):\n",
        "    Complaint_Type: Literal[\"Data\", \"Voice\", \"Billing\", \"Package\"]\n",
        "    Location: str\n",
        "    Handset_Model: Union[str, int]\n",
        "    Balance_Available : bool\n",
        "\n",
        "# Define the logic to call the model\n",
        "def User_Input(state: State) -> State:\n",
        "  State.Complaint_Type = input(\" Please Enter Your problem. Mention if it is related to Data / Voice / Billing / Package \")\n",
        "  print(\"You entered:\", State.Complaint_Type)\n",
        "\n",
        "\n",
        "# Define the logic to call the model\n",
        "def Balance_Check(state: State) -> State:\n",
        "  State.Phone_Number = input(\" Please share your phone number?\")\n",
        "  if State.Phone_Number == \"923013344555\":\n",
        "    State.Balance_Available = 1\n",
        "    print(\"Your Balance is available\")\n",
        "  else:\n",
        "    State.Balance_Available = 0\n",
        "    print(\"Please load balance and then call again\")\n",
        "\n",
        "\n",
        "Balance_Check(State)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuQpOIwW_Van",
        "outputId": "c4b17f2c-4596-4820-f0c4-501d4c736f3d"
      },
      "id": "OuQpOIwW_Van",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Please share your phone number?9232156847\n",
            "Please load balance and then call again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d8cb629-213f-4b87-965e-19b812c42da1",
      "metadata": {
        "id": "9d8cb629-213f-4b87-965e-19b812c42da1"
      },
      "source": [
        "Let's re-define our chatbot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "dc414e29-2078-41a0-887c-af1a6a3d72c0",
      "metadata": {
        "id": "dc414e29-2078-41a0-887c-af1a6a3d72c0"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
        "from typing import Literal, Union\n",
        "from langgraph.graph import END\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "model: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(model = \"gemini-1.5-flash\")\n",
        "\n",
        "class State(MessagesState):\n",
        "    Complaint_Type: Literal[\"Data\", \"Voice\", \"Billing\", \"Package\"]\n",
        "    Location: str\n",
        "    Handset_Model: Union[str, int]\n",
        "\n",
        "\n",
        "\n",
        "# Define the logic to call the model\n",
        "def call_model(state: State) -> State:\n",
        "\n",
        "    # Get summary if it exists\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "\n",
        "    # If there is summary, then we add it\n",
        "    if summary:\n",
        "\n",
        "        # Add summary to system message\n",
        "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
        "\n",
        "        # Append summary to any newer messages\n",
        "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
        "\n",
        "    else:\n",
        "        messages = state[\"messages\"]\n",
        "\n",
        "    response = model.invoke(messages)\n",
        "    return {\"messages\": response}\n",
        "\n",
        "def summarize_conversation(state: State) -> State:\n",
        "\n",
        "    # First, we get any existing summary\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "\n",
        "    # Create our summarization prompt\n",
        "    if summary:\n",
        "\n",
        "        # A summary already exists\n",
        "        summary_message = (\n",
        "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
        "            \"Extend the summary by taking into account the new messages above:\"\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        summary_message = \"Create a summary of the conversation above:\"\n",
        "\n",
        "    # Add prompt to our history\n",
        "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
        "    response = model.invoke(messages)\n",
        "\n",
        "    # Delete all but the 2 most recent messages\n",
        "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
        "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
        "\n",
        "# Determine whether to end or summarize the conversation\n",
        "def should_continue(state: State) -> State:\n",
        "\n",
        "    \"\"\"Return the next node to execute.\"\"\"\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # If there are more than six messages, then we summarize the conversation\n",
        "    if len(messages) > 6:\n",
        "        return \"summarize_conversation\"\n",
        "\n",
        "    # Otherwise we can just end\n",
        "    return END"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41c13c0b-a383-4f73-9cc1-63f0eed8f190",
      "metadata": {
        "id": "41c13c0b-a383-4f73-9cc1-63f0eed8f190"
      },
      "source": [
        "Now, we just re-compile with our sqlite checkpointer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e867fd95-91eb-4ce1-82fc-bb72d611a96d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "e867fd95-91eb-4ce1-82fc-bb72d611a96d",
        "outputId": "89530f09-b075-4bd8-a2f5-80a33bf19a73"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (<ipython-input-11-2ae4435bbef9>, line 11)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-2ae4435bbef9>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    workflow.add_node(Solution Fetching)\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "from langgraph.graph import StateGraph, START\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "\n",
        "# Define a new graph\n",
        "workflow: StateGraph = StateGraph(State)\n",
        "workflow.add_node(\"Human_Message_Input\",User_Input)\n",
        "workflow.add_node(User_Authentication)\n",
        "workflow.add_node(\"Package Explainer\",Packager)\n",
        "workflow.add_node(Details_Fetching)\n",
        "workflow.add_node(Solution Fetching)\n",
        "workflow.add_node(Solution Explainer)\n",
        "\n",
        "\n",
        "\n",
        "# Set the entrypoint as conversation\n",
        "workflow.add_edge(START, Human_Message_Input)\n",
        "workflow.add_edge(Human_Message_Input, User_Authentication)\n",
        "workflow.add_conditional_edges(User_Authentication, Decide)\n",
        "workflow.add_edge(Details_Fetching, Solution Fetching)\n",
        "workflow.add_edge(Solution Fetching, Solution Explainer)\n",
        "workflow.add_edge(Solution Explainer, END)\n",
        "\n",
        "# Compile\n",
        "graph: CompiledStateGraph = workflow.compile(checkpointer=memory)\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8769db99-3938-45e6-a594-56beb18d6c45",
      "metadata": {
        "id": "8769db99-3938-45e6-a594-56beb18d6c45"
      },
      "source": [
        "Now, we can invoke the graph several times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f4094a0-d240-4be8-903a-7d9f605bdc5c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f4094a0-d240-4be8-903a-7d9f605bdc5c",
        "outputId": "9ab4170f-6563-4fff-8533-82b32125c2a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hi Lance! It's nice to hear from you again. It seems you're a big fan of the 49ers. We were just talking about them. Is there something specific you'd like to chat about regarding the team? We could discuss their current roster, their history, or even some of their most memorable games. What interests you most about the 49ers?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Lance! You told me that at the beginning of our conversation. üòä  \n",
            "\n",
            "Do you want to talk about the 49ers? I'm happy to discuss their history, their current roster, or even some of their most memorable games.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I hear you! The 49ers have a lot of passionate fans. What specifically do you like about them? Are you a fan of their current roster, or do you have a favorite player from the past?\n"
          ]
        }
      ],
      "source": [
        "# Create a thread\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# Start conversation\n",
        "input_message = HumanMessage(content=\"hi! I'm Lance\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()\n",
        "\n",
        "input_message = HumanMessage(content=\"what's my name?\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()\n",
        "\n",
        "input_message = HumanMessage(content=\"i like the 49ers!\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0f3e842-4497-45e2-a924-69672a9bcb33",
      "metadata": {
        "id": "c0f3e842-4497-45e2-a924-69672a9bcb33"
      },
      "source": [
        "Let's confirm that our state is saved locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2ab158a-5a82-417a-8841-730a4cc18ea7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2ab158a-5a82-417a-8841-730a4cc18ea7",
        "outputId": "4c173155-1d7d-4fae-8757-9c833eeab4ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content=\"hi! I'm Lance\", additional_kwargs={}, response_metadata={}, id='281ecfdc-86c1-4ca5-9c4b-41ce2b48966e'), AIMessage(content=\"Hi Lance! It's nice to hear from you again. It seems you're a big fan of the 49ers. We were just talking about them. Is there something specific you'd like to chat about regarding the team? We could discuss their current roster, their history, or even some of their most memorable games. What interests you most about the 49ers? \\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-1b4d4c26-8466-477b-a102-58b8e7983635-0', usage_metadata={'input_tokens': 350, 'output_tokens': 80, 'total_tokens': 430}), HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='31e9889c-f54c-40a7-a4e1-83cfccc442b2'), AIMessage(content=\"Your name is Lance! You told me that at the beginning of our conversation. üòä  \\n\\nDo you want to talk about the 49ers? I'm happy to discuss their history, their current roster, or even some of their most memorable games. \\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-8cfbf18d-553b-4e93-8fad-966e0819856c-0', usage_metadata={'input_tokens': 282, 'output_tokens': 53, 'total_tokens': 335}), HumanMessage(content='i like the 49ers!', additional_kwargs={}, response_metadata={}, id='05b52815-f495-452b-9976-34d6440a1d46'), AIMessage(content='I hear you! The 49ers have a lot of passionate fans. What specifically do you like about them? Are you a fan of their current roster, or do you have a favorite player from the past? \\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-f5ab9045-2b13-4192-9667-3646a8839bbf-0', usage_metadata={'input_tokens': 347, 'output_tokens': 44, 'total_tokens': 391})], 'summary': \"Here's an extended summary of the conversation:\\n\\nLance introduced himself twice during the conversation, expressing his fondness for the San Francisco 49ers football team. The AI assistant acknowledged Lance's name each time and showed willingness to discuss the 49ers, offering to talk about various aspects of the team such as their history, current roster, or memorable games.  The AI then provided a summary of the conversation so far, highlighting Lance's repeated introductions and lack of direct engagement with the AI's prompts about the 49ers. \\n\\nThe conversation was brief and somewhat repetitive, with Lance reintroducing himself at the end without directly responding to the AI's questions or prompts about the 49ers. It seems Lance may be struggling to engage in a back-and-forth conversation or may be unsure how to respond to the AI's prompts. \\n\"}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef7b327-89bb-6c88-801b-c0cb386f00af'}}, metadata={'source': 'loop', 'writes': {'conversation': {'messages': AIMessage(content='I hear you! The 49ers have a lot of passionate fans. What specifically do you like about them? Are you a fan of their current roster, or do you have a favorite player from the past? \\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-f5ab9045-2b13-4192-9667-3646a8839bbf-0', usage_metadata={'input_tokens': 347, 'output_tokens': 44, 'total_tokens': 391})}}, 'step': 27, 'parents': {}}, created_at='2024-09-25T11:36:57.382795+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef7b327-855c-6983-801a-e36891ddda9b'}}, tasks=())"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "graph_state = graph.get_state(config)\n",
        "graph_state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e21152d-ed9c-408d-b7d5-f634c9ce81e2",
      "metadata": {
        "id": "1e21152d-ed9c-408d-b7d5-f634c9ce81e2"
      },
      "source": [
        "### Persisting state\n",
        "\n",
        "Using database like Sqlite means state is persisted!\n",
        "\n",
        "For example, we can re-start the notebook kernel and see that we can still load from Sqlite DB on disk.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9a44dc5-be04-45fa-a6fc-27b0f8ee4678",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9a44dc5-be04-45fa-a6fc-27b0f8ee4678",
        "outputId": "887ade22-4304-4370-c654-d42e4183c068"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content=\"hi! I'm Lance\", additional_kwargs={}, response_metadata={}, id='086e80ce-8bd0-4dee-b13c-f47160d6755f'), AIMessage(content=\"Hello Lance! You've already introduced yourself, but it's nice to greet you again. Is there something specific you'd like to talk about? Perhaps you'd like to continue our conversation about the 49ers, or is there another topic you're interested in discussing?\", additional_kwargs={}, response_metadata={'id': 'msg_01AkDF6AH7LMp2YDDD9jQkEC', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 197, 'output_tokens': 58}}, id='run-7f8af5e1-5248-4ccd-ab46-87580422f0d6-0', usage_metadata={'input_tokens': 197, 'output_tokens': 58, 'total_tokens': 255}), HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='e33f42ee-cdb0-45ec-b100-54dcd4c82e71'), AIMessage(content=\"Your name is Lance. You've introduced yourself twice during our conversation.\", additional_kwargs={}, response_metadata={'id': 'msg_01LVJSCjRjmRd3QA2UMP6SXg', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 193, 'output_tokens': 17}}, id='run-4152603e-5c5f-4a26-be8c-2b2bab373e75-0', usage_metadata={'input_tokens': 193, 'output_tokens': 17, 'total_tokens': 210}), HumanMessage(content='i like the 49ers!', additional_kwargs={}, response_metadata={}, id='85513182-bfb9-445b-b6ff-a6553bf63767'), AIMessage(content=\"Yes, you've mentioned that you like the San Francisco 49ers! They're a popular NFL team with a rich history. Since you're a fan, is there anything specific about the 49ers you'd like to discuss? We could talk about:\\n\\n1. Their current roster and key players\\n2. The team's history and past achievements\\n3. Memorable games or seasons\\n4. Their prospects for the upcoming season\\n5. The team's rivalries\\n6. Their home stadium, Levi's Stadium\\n\\nWhat aspect of the 49ers interests you most?\", additional_kwargs={}, response_metadata={'id': 'msg_018FVfpp9brQykX53VK88tv5', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 220, 'output_tokens': 123}}, id='run-68b784a6-c518-4904-a208-6214b7223414-0', usage_metadata={'input_tokens': 220, 'output_tokens': 123, 'total_tokens': 343})], 'summary': \"Here's a summary of the conversation:\\n\\nLance introduced himself twice during the conversation. He expressed his fondness for the San Francisco 49ers football team. The AI assistant acknowledged Lance's name each time and showed willingness to discuss the 49ers, offering to talk about various aspects of the team such as their history, current roster, or memorable games. The conversation was brief and somewhat repetitive, with Lance reintroducing himself at the end without directly responding to the AI's questions or prompts about the 49ers.\"}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef64b23-31cd-655a-8011-d28d83f59223'}}, metadata={'source': 'loop', 'writes': {'conversation': {'messages': AIMessage(content=\"Yes, you've mentioned that you like the San Francisco 49ers! They're a popular NFL team with a rich history. Since you're a fan, is there anything specific about the 49ers you'd like to discuss? We could talk about:\\n\\n1. Their current roster and key players\\n2. The team's history and past achievements\\n3. Memorable games or seasons\\n4. Their prospects for the upcoming season\\n5. The team's rivalries\\n6. Their home stadium, Levi's Stadium\\n\\nWhat aspect of the 49ers interests you most?\", additional_kwargs={}, response_metadata={'id': 'msg_018FVfpp9brQykX53VK88tv5', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 220, 'output_tokens': 123}}, id='run-68b784a6-c518-4904-a208-6214b7223414-0', usage_metadata={'input_tokens': 220, 'output_tokens': 123, 'total_tokens': 343})}}, 'step': 17}, created_at='2024-08-27T20:23:19.625843+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef64b23-1a48-6402-8010-79e09364e17f'}}, tasks=())"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a thread\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "graph_state = graph.get_state(config)\n",
        "graph_state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8466e418-1a46-4cdb-a51a-6ae14281bb85",
      "metadata": {
        "id": "8466e418-1a46-4cdb-a51a-6ae14281bb85"
      },
      "source": [
        "## LangGraph Studio\n",
        "\n",
        "--\n",
        "\n",
        "**‚ö†Ô∏è DISCLAIMER**\n",
        "\n",
        "*Running Studio currently requires a Mac. If you are not using a Mac, then skip this step.*\n",
        "\n",
        "--\n",
        "\n",
        "Now that we better understand external memory, recall that the LangGraph API packages your code and provides you with with built-in persistence.\n",
        "\n",
        "And the API is the back-end for Studio!\n",
        "\n",
        "Load the `chatbot` in the UI, which uses `module2-/studio/chatbot.py` set in `module2-/studio/langgraph.json`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4916d8b",
      "metadata": {
        "id": "c4916d8b"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}